{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utility functions useful for analysis\"\"\"\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "import squarify\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonascii(words):\n",
    "    return [w for w in words if w in string.printable]\n",
    "\n",
    "\n",
    "def remove_stop(words):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return [w for w in words if not w in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "\n",
    "def remove_punct(words):\n",
    "    return [w for w in words if not w in string.punctuation]\n",
    "    \n",
    "    \n",
    "def lowercase(words):\n",
    "    return [w.lower() for w in words]\n",
    "\n",
    "\n",
    "def remove_short(words):\n",
    "    return [w for w in words if len(w) > 2]\n",
    "\n",
    "\n",
    "def filter_pos(words, pos):\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    pos = \" \".join(pos)\n",
    "    return [w[0] for w in tagged if w[1] in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s, pos=None):\n",
    "    s = contractions.fix(s)\n",
    "    words = word_tokenize(s)\n",
    "    \n",
    "    if pos:\n",
    "        words = filter_pos(words, pos)\n",
    "    \n",
    "    words = lowercase(words)\n",
    "    words = lemmatize(words)\n",
    "    words = remove_stop(words)\n",
    "    words = remove_punct(words)\n",
    "    words = remove_short(words)\n",
    "    \n",
    "    assert(words is not None)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    print(\"Loading Glove Model\")\n",
    "    with open(path,'r', encoding='utf8') as f:\n",
    "        embs = {}\n",
    "        for line in tqdm_notebook(f):\n",
    "            splitLine = line.split(' ')\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            embs[word] = embedding\n",
    "            \n",
    "    print(\"Done.\",len(embs),\" words loaded!\")\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_string(s, glove):\n",
    "    s = contractions.fix(s)\n",
    "    \n",
    "    words = word_tokenize(s)\n",
    "    words = lowercase(words)\n",
    "    \n",
    "    emb = np.zeros(glove['hi'].shape)\n",
    "    for w in words:\n",
    "        try:\n",
    "            emb += glove[w]\n",
    "            \n",
    "        except KeyError:\n",
    "            # word not found in glove\n",
    "            continue\n",
    "        \n",
    "    return emb / len(words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(emb1, emb2):\n",
    "    return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to count the words\n",
    "def count(words):\n",
    "    counts = {}\n",
    "    unique = set(words)\n",
    "    for w in unique:\n",
    "        counts[w] = words.count(w)\n",
    "    \n",
    "#     _ = counts.pop(\"hence\")\n",
    "    sorted_counts = sorted(zip(counts.keys(), counts.values()), key=lambda x: x[1], reverse=True)\n",
    "    labels, counts = zip(*sorted_counts)\n",
    "    return labels, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to count the words\n",
    "def count_dict(words):\n",
    "    counts = {}\n",
    "    unique = set(words)\n",
    "    for w in unique:\n",
    "        counts[w] = words.count(w)\n",
    "    \n",
    "#     _ = counts.pop(\"hence\")\n",
    "    sorted_counts = sorted(zip(counts.keys(), counts.values()), key=lambda x: x[1], reverse=True)\n",
    "    return {word: count for word, count in sorted_counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the words \n",
    "def plot_squarify(words, show):\n",
    "    labels, sizes = count(words)\n",
    "    \n",
    "    _labels = []\n",
    "    for i in range(show):\n",
    "        _labels.append(labels[i]+\" - \"+str(sizes[i]))\n",
    "\n",
    "    plt.rcParams.update({'font.size':36})\n",
    "    fig=plt.gcf()\n",
    "    fig.set_size_inches(40,15)\n",
    "    squarify.plot(sizes=sizes[:show],\n",
    "                  label=_labels[:show],\n",
    "                  color=sns.color_palette('GnBu_r',show+11)[11:],\n",
    "                  alpha=0.9)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_freqs(words1, words2, n=15):\n",
    "    words1, counts1 = count(words1)\n",
    "    words2, counts2 = count(words2)\n",
    "    \n",
    "    count_dict1 = {word: count for word, count in zip(words1, counts1)}\n",
    "    count_dict2 = {word: count for word, count in zip(words2, counts2)}\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(30,14)\n",
    "    \n",
    "    texts = []\n",
    "    for i in range(1, n):\n",
    "        word = words1[i]\n",
    "        noise = np.random.normal(size=2)\n",
    "        f1 = count_dict1.get(word, 0) + noise[0]\n",
    "        f2 = count_dict2.get(word, 0) + noise[1]\n",
    "        ax.scatter(f1, f2, c='#008fd5', s=100)\n",
    "        texts.append(plt.text(f1, f2, word, fontsize=27))\n",
    "        \n",
    "    for i in range(n):\n",
    "        word = words2[i]\n",
    "        if word in words1[:n]:\n",
    "            continue\n",
    "        \n",
    "        noise = np.random.normal(size=2)\n",
    "        f1 = count_dict1.get(word, 0) + noise[0]\n",
    "        f2 = count_dict2.get(word, 0) + noise[1]\n",
    "        ax.scatter(f1, f2, c='#008fd5', s=100)\n",
    "        texts.append(plt.text(f1, f2, word, fontsize=27))\n",
    "        \n",
    "    plt.xticks(fontsize=28)\n",
    "    plt.yticks(fontsize=28)\n",
    "    \n",
    "    adjust_text(texts, arrowprops=dict(arrowstyle=\"-\", color='b', lw=1.1))\n",
    "    \n",
    "    plt.plot(np.arange(0, 30, 0.1), np.arange(0, 30, 0.1), linewidth=3, c='#fc4f30', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
